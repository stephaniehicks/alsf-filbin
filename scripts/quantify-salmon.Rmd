---
title: quantification of single nuc-seq data (alsf-filbin) with salmon 
author: Stephanie Hicks
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(here)
```

This code chunk copies the data from where it is stored into the github repo so we can all work with relative paths. 
I copy it so I can keep the backed up copy where I originally have it. 
However, the person using this script needs to adjust which paths to use. 
```{r}
stephanie_data_path <- "/users/shicks1/data/alsf_filbin/sample"
# albert_data_dath <- "/users/akuo/data/alsf_filbin/sample"

if(!dir.exists(here("sample_data"))){
  dir.create(here("sample_data"))
  
  file.copy(stephanie_data_path, here("sample_data"), recursive=FALSE)
  # file.copy(albert_data_path, here("sample_data"), recursive=FALSE)
}

# verify the tumor files has been transferred. 
tumor_names <- list.files(here("sample_data"))[
                  !grepl("*.txt", list.files(here("sample_data")))]
```

Next, we create the folder to store quantification files from salmon.  
```{r}
if(!dir.exists(here("salmon_quants"))){
  dir.create(here("salmon_quants"))
}
```

List cells and tumor names 

```{r}
if(!file.exists(here("sample_data", "unique_cell_names.txt"))){
  unique_cell_names <- NULL
  for(tum in tumor_names){
    ids <- list.files(here("sample_data", tum))
    ids <- unique(stringr::str_sub(ids, end=-11))
    ids <- file.path(tum, ids)
    unique_cell_names <- c(unique_cell_names, ids)
  }
  readr::write_lines(unique_cell_names,
                     path = file.path(path_dir_home, "unique_cell_filenames.txt"))
}
```

# Quantification 

## Download files needed for Salmon

Gene transfer format (GTF) is a file format used to hold information 
about gene structure. It is a tab-delimited text format based on
the general feature format (GFF), but contains some additional 
conventions specific to gene information.

```{bash}
# hi!
cd /users/shicks1/data/reference/genomes/hsapiens/ENSEMBL/GRCh38/salmon

# download ensemble fastq cdna file 
wget ftp://ftp.ensembl.org/pub/release-98/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.all.fa.gz

# download gtf file
wget ftp://ftp.ensembl.org/pub/release-98/gtf/homo_sapiens/Homo_sapiens.GRCh38.98.gtf.gz
```

```{bash}
cd /users/shicks1/data/reference/genomes/hsapiens/GENCODE/release_32

# download GENCODE fastq DNA file 
wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_32/GRCh38.primary_assembly.genome.fa.gz

# download GENCODE fastq cdna file 
wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_32/gencode.v32.transcripts.fa.gz

# download GENCODE gtf file 
wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_32/gencode.v32.annotation.gtf.gz
```

Next, we need to create a set of pre-mRNA sequences to 
save to a FASTA file. This is needed because we have 
single nucleus RNA-seq data so we need to take into account 
the pre-mRNA and mRNA transcripts. This code kindly comes from 
[Charlotte Soneson](https://csoneson.github.io).

```{r}
suppressPackageStartupMessages({
  library(Biostrings)
  library(rtracklayer)
  library(GenomicFeatures)
  library(BSgenome)
})

gencode_path <- "/users/shicks1/data/reference/genomes/hsapiens/GENCODE/release_32"
## Get genomic (DNA) sequence
genomefasta <- file.path(gencode_path, "GRCh38.primary_assembly.genome.fa.gz")

## Import FASTA file with Biostrings::readDNAStringSet
genome <- Biostrings::readDNAStringSet(genomefasta)
names(genome) <- sapply(strsplit(names(genome), " "), .subset, 1) # creates chr1, etc

ingtf <- file.path(gencode_path, "gencode.v32.annotation.gtf.gz")

## Read gtf file and group exons by transcript
txdb <- GenomicFeatures::makeTxDbFromGFF(ingtf)
grl <- GenomicFeatures::exonsBy(txdb, by = "tx", use.names = TRUE)

## Create the pre-mRNA coordinates by adding all introns to each transcript
grrange <- unlist(range(grl))
```

Now, for each entry in `grrange`, we extract the sequence from the 
corresponding `seqnames` entry in the `genome`. 

**Note**: If you use Ensembl, there will be a discrepancy between the 
transcripts (e.g. ENST00000456328 vs ENST00000456328.1). Technically, it doesn't 
matter this point, but it will when we merge with the spliced (mRNA) 
transcripts. So, if you do this, you will likely need to remove the version
numbers before merging the pre-mRNA and mRNA fasta files. Or just use the 
Gencode annotation where the naming is consistent between the 
transcriptome fasta and the gtf.

```{r}
## Get the sequence of each pre-mRNA molecule
premrnaseq <- BSgenome::getSeq(x = genome, names = grrange)

premrna_fasta <- file.path(gencode_path, "GRCh38.premRNA.fa")

## Save pre-mRNA sequences to fasta file 
Biostrings::writeXStringSet(premrnaseq, filepath = premrna_fasta)
```

Let's first check to make sure they have the same number of transcripts 
(pre-mRNA and mRNA). 
```{bash}
zcat gencode.v32.transcripts.fa.gz | grep ">"  | wc -l
grep ">" GRCh38.premRNA.fa | wc -l
```

Both have the same number of rows (or transcripts): 227462. I think 
it's also interesting that one is almost 6GB and the other is 70M. 

Now we will put the pre-mRNA (`GRCh38.premRNA.fa`) and 
mRNA (`gencode.v32.transcripts.fa.gz`) fasta GENCODE files together. 

```{bash}
zcat gencode.v32.transcripts.fa.gz > gencode.v32.transcripts.fa
cat gencode.v32.transcripts.fa GRCh38.premRNA.fa > gencode.v32.preandmrna.fa
grep ">" gencode.v32.preandmrna.fa | wc -l
```


Ok there are 454924 (227462 / 2) transcripts, as we expect. Now let's 
zip this file up. This takes a while. 

```{bash}
gzip gencode.v32.preandmrna.fa
```

And clean up our space 
```{bash}
rm gencode.v32.transcripts.fa
rm gencode.v32.transcripts.fa.gz
```

## Install and build salmon index 

To upgrade to v1.0.0 salmon
```{bash}
wget https://github.com/COMBINE-lab/salmon/releases/download/v1.0.0/salmon-1.0.0_linux_x86_64.tar.gz
tar xzvf salmon-1.0.0_linux_x86_64.tar.gz
rm salmon-1.0.0_linux_x86_64.tar.gz
```

Also, make sure this is in the `.bash_profile` file
```{bash}
PATH=$PATH:/users/shicks1/src/salmon-latest_linux_x86_64/bin
```

You can check to make sure salmon has been upgraded correctly 
using `salmon -h` inside terminal (or help with specific parts of using 
salmong using e.g. `salmon index -h` for help with the index step). 

OK, we are ready to use salmon. The `-t` argument is the input transcripts file, 
the `-i` argument is the index file to create, the `--keepDuplicates` argument
forces  all duplicate transcripts that appear in the input will 
be retained and quantified separately, and the `--threads` argument says how 
many threads to use when building the index. 

```{bash}
cd /users/shicks1/data/reference/genomes/hsapiens/GENCODE/release_32

# create salmon index (3-5 mins)
salmon index -t gencode.v32.preandmrna.fa.gz -i gencode.v32_salmon-index-v1.0.0 --threads 4
```

## Running salmon 

See the `scripts/run-salmon.sh` file in the terminal. 

## Creating SummarizedExperiment object

```{r}
path_dir <- "/fastscratch/myscratch/shicks1/alsf-filbin"
tumor_names <- list.files(path_dir)
tumor_names <- tumor_names[!(tumor_names %in% 
                          c("Human_medians.tsv", "Kriegstein_Human_cortex", 
                            "unique_cell_names.txt", "salmon_quants"))]

unique_sf_paths <- NULL
for(tum in tumor_names){
  ids <- list.files(paste(path_dir, tum, sep="/"))
  ids <- unique(stringr::str_sub(ids, end=-13))
  ids <- paste(path_dir, "salmon_quants", paste0(ids, "_quant/quant.sf"), sep="/")
  unique_sf_paths <- c(unique_sf_paths, ids)
}

unique_sf_ids <- NULL
for(tum in tumor_names){
  ids <- list.files(paste(path_dir, tum, sep="/"))
  ids <- unique(stringr::str_sub(ids, end=-13))
  unique_sf_ids <- c(unique_sf_ids, ids)
}

coldata <- data.frame(files = unique_sf_paths, names = unique_sf_ids)

```

```{r}
library(tximeta)
se <- tximeta(coldata[1:96,])
```


# Quality control 

## Explore SummarizedExperiment object

```{r}
suppressPackageStartupMessages(library(SummarizedExperiment))
colData(se)
assayNames(se)
rowRanges(se)
```

```{r}
dat <- assay(se, "counts")
hist(colSums(dat))
```